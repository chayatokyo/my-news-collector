"""
RSS News Collector

config/ å†…ã®YAMLå®šç¾©ã«åŸºã¥ãã€RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ã—
Markdownãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚

Usage:
    python scripts/collect_rss.py --config config/ai-news.yaml [--date 2026-02-17]
"""

import argparse
import re
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any

import feedparser
import yaml

# æ—¥æœ¬æ¨™æº–æ™‚
JST = timezone(timedelta(hours=9))

# æ›œæ—¥åï¼ˆæ—¥æœ¬èªï¼‰
WEEKDAY_JA = ["æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "æ—¥"]


def load_config(config_path: str) -> dict[str, Any]:
    """YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    path = Path(config_path)
    if not path.exists():
        print(f"Error: Config file not found: {config_path}", file=sys.stderr)
        sys.exit(1)

    with open(path, encoding="utf-8") as f:
        return yaml.safe_load(f)


def parse_entry_date(entry: feedparser.FeedParserDict) -> datetime | None:
    """ãƒ•ã‚£ãƒ¼ãƒ‰ã‚¨ãƒ³ãƒˆãƒªã®å…¬é–‹æ—¥æ™‚ã‚’ãƒ‘ãƒ¼ã‚¹ã™ã‚‹"""
    for attr in ("published_parsed", "updated_parsed"):
        parsed = getattr(entry, attr, None)
        if parsed:
            try:
                return datetime(*parsed[:6], tzinfo=timezone.utc)
            except (ValueError, TypeError):
                continue
    return None


def matches_keywords(text: str, keywords: list[str]) -> bool:
    """ãƒ†ã‚­ã‚¹ãƒˆãŒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã„ãšã‚Œã‹ã«åˆè‡´ã™ã‚‹ã‹åˆ¤å®š"""
    lower_text = text.lower()
    return any(kw.lower() in lower_text for kw in keywords)


def matches_exclude_keywords(text: str, exclude_keywords: list[str]) -> bool:
    """ãƒ†ã‚­ã‚¹ãƒˆãŒé™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã„ãšã‚Œã‹ã«åˆè‡´ã™ã‚‹ã‹åˆ¤å®š"""
    if not exclude_keywords:
        return False
    lower_text = text.lower()
    return any(kw.lower() in lower_text for kw in exclude_keywords)


def fetch_single_feed(
    feed_config: dict[str, str],
) -> tuple[str, list[feedparser.FeedParserDict], str | None]:
    """å˜ä¸€ã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ã™ã‚‹ã€‚(name, entries, error) ã‚’è¿”ã™"""
    name = feed_config["name"]
    url = feed_config["url"]
    try:
        # User-Agent ã‚’è¨­å®šï¼ˆRedditç­‰ãŒãƒ–ãƒ­ãƒƒã‚¯ã™ã‚‹ãŸã‚ï¼‰
        parsed = feedparser.parse(
            url,
            agent="my-news-collector/1.0 (https://github.com/chayatokyo/my-news-collector)",
        )
        if parsed.bozo and not parsed.entries:
            return (name, [], f"Parse error: {parsed.bozo_exception}")
        return (name, parsed.entries, None)
    except Exception as e:
        return (name, [], str(e))


def collect_articles(
    config: dict[str, Any], target_date: datetime
) -> tuple[list[dict[str, str]], list[dict[str, str]]]:
    """
    å…¨ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’åé›†ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã€‚
    Returns: (articles, errors)
    """
    feeds = config.get("feeds", [])
    keywords = config.get("keywords", [])
    exclude_keywords = config.get("exclude_keywords", [])
    fetch_hours = config.get("fetch_hours", 48)
    cutoff_time = target_date - timedelta(hours=fetch_hours)

    articles: list[dict[str, str]] = []
    errors: list[dict[str, str]] = []
    seen_urls: set[str] = set()

    print(f"Fetching {len(feeds)} feeds...")

    # ä¸¦åˆ—ã§ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = {
            executor.submit(fetch_single_feed, feed): feed for feed in feeds
        }
        for future in as_completed(futures):
            feed_config = futures[future]
            name, entries, error = future.result()

            if error:
                errors.append({"name": name, "error": error})
                print(f"  âœ— {name}: {error}")
                continue

            feed_article_count = 0
            for entry in entries:
                # URL ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
                url = getattr(entry, "link", "")
                if not url or url in seen_urls:
                    continue

                # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿
                pub_date = parse_entry_date(entry)
                if pub_date and pub_date < cutoff_time:
                    continue

                # ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆã—ã¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿
                title = getattr(entry, "title", "")
                summary = getattr(entry, "summary", "")
                combined_text = f"{title} {summary}"

                # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
                if matches_exclude_keywords(combined_text, exclude_keywords):
                    continue

                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ
                if not matches_keywords(combined_text, keywords):
                    continue

                seen_urls.add(url)
                feed_article_count += 1
                articles.append(
                    {
                        "title": clean_text(title),
                        "url": url,
                        "source": name,
                        "category": feed_config.get("category", "other"),
                        "language": feed_config.get("language", "en"),
                        "published": (
                            pub_date.astimezone(JST).strftime("%Y-%m-%d %H:%M")
                            if pub_date
                            else "ä¸æ˜"
                        ),
                        "summary": clean_text(summary)[:200],
                    }
                )

            print(f"  âœ“ {name}: {feed_article_count} articles")

    # ã‚«ãƒ†ã‚´ãƒªå„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆï¼ˆå…¬å¼ â†’ å›½å†… â†’ æµ·å¤– â†’ æŠ€è¡“ â†’ Reddit â†’ æ¥­ç•Œï¼‰
    category_order = {
        "official": 0,
        "domestic": 1,
        "international": 2,
        "tech": 3,
        "reddit": 4,
        "industry": 5,
        "other": 6,
    }
    articles.sort(key=lambda a: category_order.get(a["category"], 99))

    return articles, errors


def clean_text(text: str) -> str:
    """HTMLã‚¿ã‚°é™¤å»ãƒ»ç©ºç™½æ•´ç†"""
    text = re.sub(r"<[^>]+>", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text


def category_label(category: str) -> str:
    """ã‚«ãƒ†ã‚´ãƒªåã‚’æ—¥æœ¬èªãƒ©ãƒ™ãƒ«ã«å¤‰æ›"""
    labels = {
        "official": "ğŸ¢ AIä¼æ¥­å…¬å¼",
        "domestic": "ğŸ“° å›½å†…ãƒ¡ãƒ‡ã‚£ã‚¢",
        "international": "ğŸŒ æµ·å¤–ãƒ¡ãƒ‡ã‚£ã‚¢",
        "tech": "ğŸ’» æŠ€è¡“ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£",
        "reddit": "ğŸ’¬ Reddit",
        "industry": "ğŸ‡¯ğŸ‡µ æ¥­ç•Œç‰¹åŒ–",
        "other": "ğŸ“‹ ãã®ä»–",
    }
    return labels.get(category, category)


def generate_markdown(
    config: dict[str, Any],
    articles: list[dict[str, str]],
    errors: list[dict[str, str]],
    target_date: datetime,
) -> str:
    """Markdownãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã™ã‚‹"""
    date_str = target_date.strftime("%Y-%m-%d")
    weekday = WEEKDAY_JA[target_date.weekday()]
    date_jp = f"{target_date.year}å¹´{target_date.month}æœˆ{target_date.day}æ—¥ï¼ˆ{weekday}ï¼‰"

    lines: list[str] = []
    lines.append(f"# AI News â€” {date_jp}")
    lines.append("")
    lines.append(
        f"> è‡ªå‹•åé›†: {len(articles)} ä»¶ / ã‚¨ãƒ©ãƒ¼: {len(errors)} ä»¶"
    )
    lines.append(f"> åé›†æ™‚åˆ»: {datetime.now(JST).strftime('%Y-%m-%d %H:%M JST')}")
    lines.append("")

    if not articles:
        lines.append("æœ¬æ—¥ã®è©²å½“è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        lines.append("")
    else:
        # ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        current_category = ""
        for article in articles:
            if article["category"] != current_category:
                current_category = article["category"]
                lines.append(f"## {category_label(current_category)}")
                lines.append("")

            lines.append(f"- [ ] [{article['title']} | {article['source']}]({article['url']})")
            if article["summary"]:
                lines.append(f"      {article['summary'][:150]}")
            lines.append("")

    # ã‚¨ãƒ©ãƒ¼æƒ…å ±
    if errors:
        lines.append("---")
        lines.append("")
        lines.append("## âš  å–å¾—ã‚¨ãƒ©ãƒ¼")
        lines.append("")
        for err in errors:
            lines.append(f"- **{err['name']}**: {err['error']}")
        lines.append("")

    return "\n".join(lines)


def main() -> None:
    parser = argparse.ArgumentParser(description="RSS News Collector")
    parser.add_argument(
        "--config",
        required=True,
        help="Path to config YAML file (e.g., config/ai-news.yaml)",
    )
    parser.add_argument(
        "--date",
        default=None,
        help="Target date (YYYY-MM-DD). Defaults to today (JST).",
    )
    args = parser.parse_args()

    # è¨­å®šèª­ã¿è¾¼ã¿
    config = load_config(args.config)
    collection_name = config.get("name", "default")

    # æ—¥ä»˜ã®æ±ºå®š
    if args.date:
        target_date = datetime.strptime(args.date, "%Y-%m-%d").replace(tzinfo=JST)
    else:
        target_date = datetime.now(JST)

    date_str = target_date.strftime("%Y-%m-%d")
    print(f"Collection: {collection_name}")
    print(f"Date: {date_str}")
    print()

    # è¨˜äº‹åé›†
    start_time = time.time()
    articles, errors = collect_articles(config, target_date)
    elapsed = time.time() - start_time

    print()
    print(f"Results: {len(articles)} articles collected in {elapsed:.1f}s")
    print(f"Errors: {len(errors)} feeds failed")

    # Markdown ç”Ÿæˆ
    markdown = generate_markdown(config, articles, errors, target_date)

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    output_dir = Path(config.get("output", {}).get("directory", f"output/{collection_name}"))
    output_dir.mkdir(parents=True, exist_ok=True)

    output_path = output_dir / f"{date_str}.md"
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(markdown)

    print(f"Output: {output_path}")


if __name__ == "__main__":
    main()
